{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "833d36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fitz\n",
    "import time\n",
    "import polars as pl\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d003fb",
   "metadata": {},
   "source": [
    "### Pre-processing and general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for transforming the pdf's to a readable state. \n",
    "def remove_headers_and_footers(\n",
    "    pdf_path, header_height_pt=70, footer_height_pt=70\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, excluding the header and footer areas.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        header_height_pt (int, optional): Height of the header area in points.\n",
    "            Defaults to 50 (a reasonable starting value).\n",
    "        footer_height_pt (int, optional): Height of the footer area in points.\n",
    "            Defaults to 50 (a reasonable starting value).\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text without the header and footer.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        page_rect = page.rect  # Get the page rectangle\n",
    "        page_height = page_rect.height\n",
    "        # Define the clipping rectangle, excluding header and footer\n",
    "        clip_rect = fitz.Rect(\n",
    "            page_rect.x0,\n",
    "            header_height_pt,\n",
    "            page_rect.x1,\n",
    "            page_height - footer_height_pt,\n",
    "        )\n",
    "        text += page.get_text(clip=clip_rect) + \"\\n\"  # Add newline between pages\n",
    "    doc.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "c4e1f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the path to the PDF file and merging all the pdfs\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "pdf_folder = os.path.join(notebook_dir, 'materials')\n",
    "\n",
    "pdf_file_path_1 = os.path.join(pdf_folder, 'Summer_that_never_was.pdf')\n",
    "pdf_file_path_2 = os.path.join(pdf_folder, 'Let_the_leaves_fall.pdf')\n",
    "pdf_file_path_3 = os.path.join(pdf_folder, 'No_breath_to_cry.pdf')\n",
    "pdf_file_path_4 = os.path.join(pdf_folder, 'To_bloom_below_the_web.pdf')\n",
    "\n",
    "text_1 = remove_headers_and_footers(pdf_file_path_1)\n",
    "text_2 = remove_headers_and_footers(pdf_file_path_2)\n",
    "text_3 = remove_headers_and_footers(pdf_file_path_3)\n",
    "text_4 = remove_headers_and_footers(pdf_file_path_4)\n",
    "\n",
    "extracted_text = text_1 + text_2 + text_3 + text_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "78929ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383082\n"
     ]
    }
   ],
   "source": [
    "print(len(text_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d48075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to create a embedding for an input.\n",
    "def get_embedding(text, model=\"text-embedding-004\", delay = 0.6):\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using Genai.\n",
    "\n",
    "    Args:\n",
    "    text (str): Input text.\n",
    "    model (str): Embedding model name.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    time.sleep(delay)\n",
    "    response = client.models.embed_content(\n",
    "              model=model, \n",
    "              contents=text)\n",
    "    \n",
    "    return response.embeddings[0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfaa1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function for calculating the cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compute similarity between consecutive sentences\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5716da",
   "metadata": {},
   "source": [
    "### Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "94061b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into sentences (basic split)\n",
    "sentences = extracted_text.split(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "562d85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6140 sentence embeddings.\n",
      "From 6140 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each sentence\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} sentence embeddings.\")\n",
    "print(f\"From {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "9bde0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    Computes chunking breakpoints based on similarity drops.\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): List of similarity scores between sentences.\n",
    "    method (str): 'percentile', 'standard_deviation', or 'interquartile'.\n",
    "    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation').\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices where chunk splits should occur.\n",
    "    \"\"\"\n",
    "    # Determine the threshold value based on the selected method\n",
    "    if method == \"percentile\":\n",
    "        # Calculate the Xth percentile of the similarity scores\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        # Calculate the mean and standard deviation of the similarity scores\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        # Set the threshold value to mean minus X standard deviations\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        # Calculate the first and third quartiles (Q1 and Q3)\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        # Set the threshold value using the IQR rule for outliers\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        # Raise an error if an invalid method is provided\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "\n",
    "    # Identify indices where similarity drops below the threshold value\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "# Compute breakpoints using the percentile method with a threshold of 90\n",
    "breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "d0be77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks: 5526\n",
      "\n",
      "First text chunk:\n",
      "28623811\n",
      "4649373\n",
      "The Summer \n",
      "That \n",
      "Never Was\n",
      "28623811\n",
      "4649373\n",
      "\n",
      "28623812\n",
      "4649374\n",
      "WILLOWSHORE\n",
      "WILLOWSHORE\n",
      "D9a\n",
      "D9a\n",
      "W17\n",
      "W17\n",
      "W11\n",
      "W11\n",
      "W9\n",
      "W9\n",
      "W26\n",
      "W26\n",
      "W15\n",
      "W15\n",
      "W5\n",
      "W5\n",
      "W10\n",
      "W10\n",
      "W13\n",
      "W13\n",
      "W14\n",
      "W14\n",
      "W12\n",
      "W12\n",
      "W8\n",
      "W8\n",
      "W7\n",
      "W7\n",
      "W5\n",
      "W5\n",
      "W6\n",
      "W6\n",
      "W4\n",
      "W4\n",
      "W3\n",
      "W3\n",
      "W2\n",
      "W2\n",
      "W1\n",
      "W1\n",
      "W20\n",
      "W20\n",
      "W21\n",
      "W21\n",
      "W24\n",
      "W24\n",
      "W22\n",
      "W22\n",
      "W23\n",
      "W23\n",
      "W30\n",
      "W30\n",
      "W19\n",
      "W19\n",
      "W16\n",
      "W16\n",
      "W27\n",
      "W27\n",
      "W28\n",
      "W28\n",
      "W25\n",
      "W25\n",
      "W18\n",
      "W18\n",
      "D11\n",
      "D11\n",
      "D9b\n",
      "D9b\n",
      "D8\n",
      "D8\n",
      "D7\n",
      "D7\n",
      "D6\n",
      "D6\n",
      "D4\n",
      "D4\n",
      "D5\n",
      "D5\n",
      "P\n",
      "D3\n",
      "D3\n",
      "Ceiba River\n",
      "Ceiba River\n",
      "D1\n",
      "D1\n",
      "Duyue River\n",
      "Duyue River\n",
      "Duyue River\n",
      "Duyue River\n",
      "P\n",
      "Dragonfly Creek\n",
      "Dragonfly Creek\n",
      "Gourd Lake\n",
      "Gourd Lake\n",
      "D2\n",
      "D2\n",
      "Ceiba Creek\n",
      "Ceiba Creek\n",
      "Ceiba River\n",
      "Ceiba River\n",
      " Duyue River\n",
      " Duyue River\n",
      "WILLOWSHORE\n",
      "WILLOWSHORE\n",
      "1200 FEET\n",
      "1200 FEET\n",
      "• Trail\n",
      "• Trail\n",
      "• Seasonal Creek\n",
      "• Seasonal Creek\n",
      "D12\n",
      "D12\n",
      "D13\n",
      "D13\n",
      "WILLOWSHORE HINTERLANDS\n",
      "WILLOWSHORE HINTERLANDS\n",
      "1 HEX = 2 MILES\n",
      "1 HEX = 2 MILES\n",
      "28623812\n",
      "4649374\n",
      "\n",
      "28623813\n",
      "4649375\n",
      "Paizo Inc.\n",
      "7120 185th Ave NE, Ste 120\n",
      "Redmond WA 98052 0577\n",
      "AUTHOR\n",
      "Sen H.H.S.\n",
      "ADDITIONAL WRITING\n",
      "James Jacobs\n",
      "DEVELOPER\n",
      "James Jacobs\n",
      "EDITING LEAD\n",
      "Ianara Natividad\n",
      "EDITORS\n",
      "Felix Dritz, Patrick Hurley, Avi Kool, \n",
      "Priscilla Lagares, Ianara Natividad, \n",
      "Simone D. Sallé, and Tan Shao Han\n",
      "COVER ARTIST\n",
      "Rodrigo Gonzalez Toledo\n",
      "INTERIOR ARTISTS\n",
      "Shafi Adams, Mylene Bertrand, \n",
      "Wilmar Ballespí Escarp, Robert Lazzaretti, \n",
      "Justine Nortjé, and Firat Solhan\n",
      "ART DIRECTION\n",
      "Sonja Morris\n",
      "GRAPHIC DESIGN\n",
      "Adriana Gasperi\n",
      "PUBLISHER\n",
      "Erik Mona\n",
      "ADVENTURE PATH 1 OF 4\n",
      "Campaign Overview\t\n",
      "2\n",
      "by James Jacobs\n",
      "The Summer That Never Was\t\n",
      "6\n",
      "by Sen H.H.S.\n",
      "Chapter 1: To Light the Night\t\n",
      "8\n",
      "Chapter 2: Reclaiming Willowshore\t\n",
      "26\n",
      "Chapter 3: The Willowshore Curse\t\n",
      "36\n",
      "Chapter 4: The Wall of Ghosts\t\n",
      "54\n",
      "Willowshore\t\n",
      "68\n",
      "by Sen H.H.S.\n",
      "Adventure Toolbox\t\n",
      "80\n",
      "by Sen H.H.S.\n",
      "Treasure\n",
      "Magic Items\t\n",
      "81\n",
      "Willowshore Fulus\t\n",
      "83\n",
      "Creatures\n",
      "Noppera-bo\t\n",
      "84\n",
      "Shinigami\t\n",
      "86\n",
      "Stone Spider\t\n",
      "87\n",
      "Thatchling\t\n",
      "88\n",
      "NPCs\n",
      "Granny Hu\t\n",
      "90\n",
      "Old Matsuki\t\n",
      "92\n",
      "The Summer \n",
      "That Never Was\n",
      "28623813\n",
      "4649375\n",
      "\n",
      "28623814\n",
      "4649376\n",
      "28623814\n",
      "4649376\n",
      "\n",
      "28623815\n",
      "4649377\n",
      "The Summer \n",
      "That Never \n",
      "Was\n",
      "Campaign \n",
      "Overview\n",
      "Chapter 1:  \n",
      "To Light the \n",
      "Night\n",
      "Chapter 2:  \n",
      "Reclaiming \n",
      "Willowshore\n",
      "Chapter 3:  \n",
      "The \n",
      "Willowshore \n",
      "Curse\n",
      "Chapter 4:  \n",
      "The Wall of \n",
      "Ghosts\n",
      "Willowshore\n",
      "Adventure  \n",
      "Toolbox\n",
      "Shenmen wasn’t always a haunted land.\n"
     ]
    }
   ],
   "source": [
    "def split_into_chunks(sentences, breakpoints):\n",
    "    \"\"\"\n",
    "    Splits sentences into semantic chunks.\n",
    "\n",
    "    Args:\n",
    "    sentences (List[str]): List of sentences.\n",
    "    breakpoints (List[int]): Indices where chunking should occur.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    start = 0  # Initialize the start index\n",
    "\n",
    "    # Iterate through each breakpoint to create chunks\n",
    "    for bp in breakpoints:\n",
    "        # Append the chunk of sentences from start to the current breakpoint\n",
    "        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n",
    "        start = bp + 1  # Update the start index to the next sentence after the breakpoint\n",
    "\n",
    "    # Append the remaining sentences as the last chunk\n",
    "    chunks.append(\". \".join(sentences[start:]))\n",
    "    return chunks  # Return the list of chunks\n",
    "\n",
    "# Create chunks using the split_into_chunks function\n",
    "text_chunks = split_into_chunks(sentences, breakpoints)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Number of semantic chunks: {len(text_chunks)}\")\n",
    "\n",
    "# Print the first chunk to verify the result\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "7b31ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    Creates embeddings for each text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    # Generate embeddings for each text chunk using the get_embedding function\n",
    "    return [get_embedding(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Create chunk embeddings using the create_embeddings function\n",
    "chunk_embeddings = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07e626",
   "metadata": {},
   "source": [
    "### Fixed length chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "09be3b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antal chunks: 967.\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "n = 2000\n",
    "overlap = 400\n",
    "\n",
    "for i in range (0, len(extracted_text), n -overlap): \n",
    "    chunks.append(extracted_text[i:i+n])\n",
    "\n",
    "print(f\"Antal chunks: {len(chunks)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "1ac4f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_length_embeddings = create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb45e2",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd17e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    #tagit bort np.array framför embedding i append\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def semantic_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\"text\": self.texts[idx],\n",
    "                            \"metadata\": self.metadata[idx],\n",
    "                            \"similarity\": score\n",
    "                            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, filename):\n",
    "        df = pl.DataFrame(\n",
    "            dict(\n",
    "                vectors=self.vectors,\n",
    "                texts=self.texts,\n",
    "                metadata=self.metadata))\n",
    "        df.write_parquet(filename)\n",
    "\n",
    "    def load(self, file):\n",
    "        df = pl.read_parquet(file, columns=[\"vectors\", \"texts\", \"metadata\"])\n",
    "        self.vectors = df[\"vectors\"].to_list()\n",
    "        self.texts = df[\"texts\"].to_list()\n",
    "        self.metadata = df[\"metadata\"].to_list()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "fa117199",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    vector_store.add_item(text=text_chunks[i], embedding=chunk_embeddings[i], metadata={\"type\": \"chunk\", \"index\": i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "eb1c1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save(\"embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "2042f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_2 = VectorStore()\n",
    "vector_store_2.load(\"embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "8f0ac5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_fixed_length = VectorStore()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    vector_store_fixed_length.add_item(text=chunks[i], embedding=fixed_length_embeddings[i], metadata={\"type\": \"chunk\", \"index\": i})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "188642e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_fixed_length.save(\"embeddings_fixed_length.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "fc3e2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_fixed_length_2 = VectorStore()\n",
    "vector_store_fixed_length_2.load(\"embeddings_fixed_length.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b57311",
   "metadata": {},
   "source": [
    "### Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "ae726427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "2e887f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(query):\n",
    "    context = \"\\n\".join(semantic_search(query, vector_store_2.texts, vector_store_2.vectors))\n",
    "    user_prompt = f\"The question is {query}. This is the context: {context}.\"\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the system prompt for the AI assistant\n",
    "\n",
    "system_prompt = \"You are an AI assistant that gives guidence based on the given context. The context is an adventure for a TTRPG called 'Pathfinder 2e'. Give fleshed out answers point by point, but make sure to indicate what part's you are less certain about.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247eda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the generate respone function.\n",
    "def generate_response(system_prompt, user_message, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        config=types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt),\n",
    "        contents=generate_user_prompt(user_message)\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "115cfa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=3)\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "a376556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, based on the context you provided, here's how long it takes to walk the Pilgrim's Path, broken down for clarity:\n",
      "\n",
      "*   **Mandatory Duration:** The journey along the Pilgrim's Path to the monastery *must* take **four days and three nights**. This is non-negotiable due to the traditions associated with the path.\n",
      "\n",
      "*   **Reason for the Duration:** The text states that those seeking knowledge traditionally stopped to spend the night three times at shrines along the way. This implies that there are specific shrines or locations where travelers are expected to pause.\n",
      "\n",
      "*   **Distance:** The actual hiking distance of the last leg of the path from Willowshore to the monastery is 13 miles. This is relatively short but the time taken is longer.\n",
      "\n",
      "*   **Speed Restriction:** Travel along the Pilgrim’s Path can’t be rushed. This reinforces that the journey is about the experience and reflection, not speed.\n",
      "\n",
      "**In summary,** regardless of the actual distance, the Pilgrim's Path requires a commitment of **four days and three nights** to complete due to its traditional and potentially magical nature.\n",
      "\n",
      "**Uncertainties:** I'm less certain about the exact nature of the shrines. The text only mentions that they are points where travelers traditionally spend the night.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(system_prompt, \"How long does it take to walk the Pilgrims path?\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085461fb",
   "metadata": {},
   "source": [
    "### Search with fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "102797c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt_fixed_length(query):\n",
    "    context = \"\\n\".join(semantic_search(query, vector_store_fixed_length_2.texts, vector_store_fixed_length_2.vectors))\n",
    "    user_prompt = f\"The question is {query}. This is the context: {context}.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "22618daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_fixed_length(system_prompt, user_message, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        config=types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt),\n",
    "        contents=generate_user_prompt(user_message)\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "e4bc0da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, based on the context provided, here's the breakdown of how long it takes to walk the Pilgrim's Path:\n",
      "\n",
      "*   **Minimum Time:** 4 days and 3 nights. The text explicitly states that to reach the ruined monastery, travelers *must* spend this amount of time following the Pilgrim's Path traditions.\n",
      "*   **Reasoning:** The tradition involves pausing to spend the night at three shrines along the 13-mile hike from Willowshore to the monastery. This breaks the journey into four segments (the stretches between Willowshore and the first shrine, between the shrines, and from the last shrine to the monastery), each taking approximately a day.\n",
      "*   **Unimpeded Travel:** While travel along the path is unimpeded in terms of obstacles or dangers (as long as you stay on the path), it *cannot* be rushed. This reinforces the 4-day/3-night minimum.\n",
      "\n",
      "**In summary, it takes a minimum of 4 days and 3 nights to walk the Pilgrim's Path to reach the ruined monastery, adhering to the traditional practice of stopping at the shrines.**\n"
     ]
    }
   ],
   "source": [
    "print(generate_response_fixed_length(system_prompt, \"How long does it take to walk the Pilgrims path?\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4fb9b",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "e80de757",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = [\n",
    "    {\n",
    "        \"question\": \"What faction does Old Matsuki represent?\",\n",
    "        \"ideal_answer\": \"Southbank\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who is the doctor in Willowshore?\",\n",
    "        \"ideal_answer\": \"Dr Dami\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What happens when the Eternal Lantern is lit?\",\n",
    "        \"ideal_answer\": \"The monsters and perils abate, citizens come out of hiding but remain frienghtened. Gray Butcher and Mo Douqiu realize that their grip on Willowshore has grown tenuous. Gray Butcher takes to patrolling downtown's streets.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "401a13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_system_prompt = \"\"\"You are an intelligent evaluationsystem with the purpose of evaluating the answer of an AI-assistent. If the answer if close to the ideal answer, score it 1.0. If it's wrong or not good enough score it 0. If it's partly correct score it 0.5. Motivate the score you give it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "e4bdc072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response correctly identifies several clues about Old Matsuki's potential allegiances, including his opposition to Lung Wa and Northridge, his leadership position in South Willowshore, and his fondness for tradition. However, it doesn't definitively state that he represents the Southbank faction.\n",
      "Therefore, I rate this answer 0.5.\n"
     ]
    }
   ],
   "source": [
    "query = validation_data[0][\"question\"]\n",
    "response = generate_response(system_prompt, query)\n",
    "\n",
    "evaluation_prompt = f\"\"\"Question: {query}\n",
    "Response: {response.text}\n",
    "Ideal answer: {validation_data[0][\"ideal_answer\"]}\"\"\"\n",
    "\n",
    "evaluation_response = generate_response(evaluation_system_prompt, evaluation_prompt)\n",
    "\n",
    "print(evaluation_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c533686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
