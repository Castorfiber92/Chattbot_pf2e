{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fitz\n",
    "import time\n",
    "import polars as pl\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d003fb",
   "metadata": {},
   "source": [
    "### Pre-processing and general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers_and_footers(\n",
    "    pdf_path, header_height_pt=70, footer_height_pt=70\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, excluding the header and footer areas.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        header_height_pt (int, optional): Height of the header area in points.\n",
    "            Defaults to 70 (a reasonable starting value).\n",
    "        footer_height_pt (int, optional): Height of the footer area in points.\n",
    "            Defaults to 70 (a reasonable starting value).\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text without the header and footer.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        page_rect = page.rect  # Get the page rectangle\n",
    "        page_height = page_rect.height\n",
    "        # Define the clipping rectangle, excluding header and footer\n",
    "        clip_rect = fitz.Rect(\n",
    "            page_rect.x0,\n",
    "            header_height_pt,\n",
    "            page_rect.x1,\n",
    "            page_height - footer_height_pt,\n",
    "        )\n",
    "        text += page.get_text(clip=clip_rect) + \"\\n\"  # Add newline between pages\n",
    "    doc.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d48075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-004\", delay = 0.6):\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using Genai.\n",
    "\n",
    "    Args:\n",
    "    text (str): Input text.\n",
    "    model (str): Embedding model name.\n",
    "    Delay (float): Seconds to wait.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    time.sleep(delay)\n",
    "    response = client.models.embed_content(\n",
    "              model=model, \n",
    "              contents=text)\n",
    "    \n",
    "    return response.embeddings[0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    Creates embeddings for each text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [get_embedding(chunk) for chunk in text_chunks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfaa1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    Computes chunking breakpoints based on similarity drops.\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): List of similarity scores between sentences.\n",
    "    method (str): 'percentile', 'standard_deviation', or 'interquartile'.\n",
    "    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation').\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices where chunk splits should occur.\n",
    "    \"\"\"\n",
    "    # Determine the threshold value based on the selected method\n",
    "    if method == \"percentile\":\n",
    "        # Calculate the Xth percentile of the similarity scores\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        # Calculate the mean and standard deviation of the similarity scores\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        # Set the threshold value to mean minus X standard deviations\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        # Calculate the first and third quartiles (Q1 and Q3)\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        # Set the threshold value using the IQR rule for outliers\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        # Raise an error if an invalid method is provided\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "\n",
    "    # Identify indices where similarity drops below the threshold value\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53875aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(sentences, breakpoints):\n",
    "    \"\"\"\n",
    "    Splits sentences into semantic chunks.\n",
    "\n",
    "    Args:\n",
    "    sentences (List[str]): List of sentences.\n",
    "    breakpoints (List[int]): Indices where chunking should occur.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    start = 0  # Initialize the start index\n",
    "\n",
    "    # Iterate through each breakpoint to create chunks\n",
    "    for bp in breakpoints:\n",
    "        # Append the chunk of sentences from start to the current breakpoint\n",
    "        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n",
    "        start = bp + 1  # Update the start index to the next sentence after the breakpoint\n",
    "\n",
    "    # Append the remaining sentences as the last chunk\n",
    "    chunks.append(\". \".join(sentences[start:]))\n",
    "    return chunks  # Return the list of chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(query, texts, embeddings):\n",
    "    \"\"\"\n",
    "    Generates a promt by comparing the query with the stored embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    str: The query and context as a prompt.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join(semantic_search(query, texts, embeddings))\n",
    "    user_prompt = f\"The question is {query}. This is the context: {context}.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ac74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_prompt, user_message, texts, embeddings, model=\"gemini-2.0-flash\", ):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    texts (List[str]): The list of text-chunks to be compared with.\n",
    "    embeddings (List[np.ndarray]): The embeddings for the text-chunks to be compared with.\n",
    "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        config=types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt),\n",
    "        contents=generate_user_prompt(user_message, texts, embeddings)\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553cd75",
   "metadata": {},
   "source": [
    "### Converting pdfs and merging to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87abbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the path to the PDF file and merging all the pdfs\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "pdf_folder = os.path.join(notebook_dir, 'materials')\n",
    "\n",
    "pdf_file_path_1 = os.path.join(pdf_folder, 'Summer_that_never_was.pdf')\n",
    "pdf_file_path_2 = os.path.join(pdf_folder, 'Let_the_leaves_fall.pdf')\n",
    "pdf_file_path_3 = os.path.join(pdf_folder, 'No_breath_to_cry.pdf')\n",
    "pdf_file_path_4 = os.path.join(pdf_folder, 'To_bloom_below_the_web.pdf')\n",
    "\n",
    "text_1 = remove_headers_and_footers(pdf_file_path_1)\n",
    "text_2 = remove_headers_and_footers(pdf_file_path_2)\n",
    "text_3 = remove_headers_and_footers(pdf_file_path_3)\n",
    "text_4 = remove_headers_and_footers(pdf_file_path_4)\n",
    "\n",
    "extracted_text = text_1 + text_2 + text_3 + text_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into sentences (basic split)\n",
    "sentences = extracted_text.split(\". \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5edd0",
   "metadata": {},
   "source": [
    "### Making the embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbdf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each sentence\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "print(f\"Generated {len(embeddings)} sentence embeddings.\")\n",
    "print(f\"From {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute breakpoints using the percentile method with a threshold of 90\n",
    "# Compute similarity between consecutive sentences\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]\n",
    "\n",
    "breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks using the split_into_chunks function\n",
    "text_chunks = split_into_chunks(sentences, breakpoints)\n",
    "\n",
    "chunk_embeddings = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07e626",
   "metadata": {},
   "source": [
    "### Making the embeddings for Fixed length chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a empty list for the text chunks.\n",
    "chunks = []\n",
    "\n",
    "# Characters in each chunk\n",
    "n = 2000\n",
    "\n",
    "# Overlap between chunks\n",
    "overlap = 400\n",
    "\n",
    "# Add chunks to the empty list\n",
    "for i in range (0, len(extracted_text), n -overlap): \n",
    "    chunks.append(extracted_text[i:i+n])\n",
    "\n",
    "print(f\"Antal chunks: {len(chunks)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the fixed length chunks\n",
    "fixed_length_embeddings = create_embeddings(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb45e2",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd17e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the VectorStore.\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def semantic_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\"text\": self.texts[idx],\n",
    "                            \"metadata\": self.metadata[idx],\n",
    "                            \"similarity\": score\n",
    "                            })\n",
    "        return results\n",
    "    \n",
    "    def save(self, filename):\n",
    "        df = pl.DataFrame(\n",
    "            dict(\n",
    "                vectors=self.vectors,\n",
    "                texts=self.texts,\n",
    "                metadata=self.metadata))\n",
    "        df.write_parquet(filename)\n",
    "\n",
    "    def load(self, file):\n",
    "        df = pl.read_parquet(file, columns=[\"vectors\", \"texts\", \"metadata\"])\n",
    "        self.vectors = df[\"vectors\"].to_list()\n",
    "        self.texts = df[\"texts\"].to_list()\n",
    "        self.metadata = df[\"metadata\"].to_list()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802cc776",
   "metadata": {},
   "source": [
    "#### Saving the semantic seartch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa117199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vectorstore object for semantic chunks. \n",
    "vector_store_s_search = VectorStore()\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    vector_store_s_search.add_item(text=text_chunks[i], embedding=chunk_embeddings[i], metadata={\"type\": \"chunk\", \"index\": i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "vector_store_s_search.save(\"embeddings_s_search.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175281b",
   "metadata": {},
   "source": [
    "#### Saving the fixed length model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ac5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate vectorstore object for the fixed length chunks\n",
    "vector_store_fixed_length = VectorStore()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    vector_store_fixed_length.add_item(text=chunks[i], embedding=fixed_length_embeddings[i], metadata={\"type\": \"chunk\", \"index\": i})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188642e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "vector_store_fixed_length.save(\"embeddings_fixed_length.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273e2dc",
   "metadata": {},
   "source": [
    "#### Loading both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_s_search = VectorStore()\n",
    "vector_store_s_search.load(\"embeddings_s_search.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_fixed_length = VectorStore()\n",
    "vector_store_fixed_length.load(\"embeddings_fixed_length.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b57311",
   "metadata": {},
   "source": [
    "### Using the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0ace21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that gives guidence based on the given context. The context is an adventure for a TTRPG called 'Pathfinder 2e'. Give fleshed out answers point by point if there is more information, but make sure to indicate what part's you are less certain about.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a376556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, based on the information you've given me, here's what I can gather about Heh Shan-Bao:\n",
      "\n",
      "*   **Scholar, Not a Fighter:** This is explicitly stated. They are more knowledgeable than combat-oriented.\n",
      "\n",
      "*   **Multiple Instances/Forms:** The phrase \"each one's face is unmistakably that of Heh Shan-Bao\" coupled with \"They move more like spiders than humans\" strongly suggests that Heh Shan-Bao exists in multiple forms or instances simultaneously. These forms are potentially spider-like in movement.\n",
      "\n",
      "*   **Escape from a Mindscape:** Heh Shan-Bao was able to escape a mindscape that traps others. This suggests they possess unique abilities or knowledge related to mental planes or psychic phenomena.\n",
      "\n",
      "*   **Connection to a Previous Life:** Carrying a link to Heh Shan-Bao's previous life is a method to interact with them or perhaps even rescue/summon them. This implies reincarnation or a strong connection between past and present lives.\n"
     ]
    }
   ],
   "source": [
    "#Test with semantic search\n",
    "query = \"What can you tell me about Heh Shan-Bao?\"\n",
    "print(generate_response(system_prompt, query, vector_store_s_search.texts, vector_store_s_search.vectors).text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with fixed length\n",
    "query = \"Tell me a joke with the context in mind\"\n",
    "print(generate_response(system_prompt, query, vector_store_fixed_length.texts, vector_store_fixed_length.vectors).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4fb9b",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80de757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list with questions for validation of the model.\n",
    "validation_data = [\n",
    "    {\n",
    "        \"question\": \"What faction does Old Matsuki represent?\",\n",
    "        \"ideal_answer\": \"Southbank\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What NPC's help the PC's when researching the willowshore curse?\",\n",
    "        \"ideal_answer\": \"You So-Jin, Igawa Jubei, Great Willow, \"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What happens when the Eternal Lantern is lit?\",\n",
    "        \"ideal_answer\": \"The monsters and perils abate, citizens come out of hiding but remain frienghtened. Gray Butcher and Mo Douqiu realize that their grip on Willowshore has grown tenuous. Gray Butcher takes to patrolling downtown's streets.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What major sprit powers can the PC's choose between when they transmigrate?\",\n",
    "        \"Ideal_answer\": \"Chilling paralysis, Dematerialize, Flickering Figure, Flight, Grudge, Resist death, River's refletion, Sap life, Undying flesh\"\n",
    "    },\n",
    "        {\n",
    "        \"question\": \"What is the purpose of 'The fang and key'?\",\n",
    "        \"Ideal_answer\": \"With the fang and key the players become able to travel to Heh Shan-Bao's domain. It also works as a everburning torch.\"\n",
    "        }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a evaluation prompt for the validation\n",
    "evaluation_system_prompt = \"\"\"You are an intelligent evaluationsystem with the purpose of evaluating the answer of an AI-assistent. You score the answer with a number between 0 and 1. If the answer if close to the ideal answer, score it 1.0. If it's wrong or not good enough score it 0.  Motivate the score you give it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bdc072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of 'The fang and key'?\n",
      "Based on the context provided, the purpose of the \"Fang and Key\" is multifaceted and crucial to the adventure:\n",
      "\n",
      "*   **Unlocking Heh Shan-Bao's Domain:** This is the most direct purpose. The Fang and Key is essential for accessing Heh Shan-Bao's domain, a significant location for the adventure.\n",
      "*   **Accessing Vital Story Content:** Without the Fang and Key, the PCs will miss significant portions of the adventure, implying important clues, encounters, or story developments reside within the areas it unlocks.\n",
      "*   **Entering the Restored Manor:** Specifically, the Fang and Key is required to enter the restored manor. This location is crucial for finding new clues about Heh Shan-Bao's presence.\n",
      "*   **Obtaining the Jorogumo Court's Mandate:** The Fang and Key symbolizes the official mandate from the jorogumo court. This mandate is essential for defeating Heh Shan-Bao and preventing Kugaptee's reincarnation.\n",
      "\n",
      "In summary, the Fang and Key is not just a physical key but also a symbolic one, representing both access to important locations and the authority needed to complete the adventure's main objectives.\n",
      "The response is quite comprehensive and captures many of the key purposes of the Fang and Key as described in the provided text. However, it misses the crucial detail that the Fang and Key symbolizes the jorogumo court's official mandate, which is essential for defeating Heh Shan-Bao and preventing Kugaptee's reincarnation.\n",
      "\n",
      "Therefore, while the answer is good, it's not perfect.\n",
      "\n",
      "Score: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell for running code to ask questions to the semantic chunking model. Change # in validation data to number between 0 and 4 for diffrent questions.\n",
    "\n",
    "query = validation_data[4][\"question\"]\n",
    "print(query)\n",
    "\n",
    "response_s_search = generate_response(system_prompt, query, vector_store_s_search.texts, vector_store_s_search.vectors)\n",
    "print(response_s_search.text)\n",
    "\n",
    "evaluation_prompt = f\"\"\"Question: {query}\n",
    "Response with semantic chunking: {response_s_search.text}\n",
    "Ideal answer: {validation_data[0][\"ideal_answer\"]}\"\"\"\n",
    "\n",
    "evaluation_response = generate_response(evaluation_system_prompt, evaluation_prompt, vector_store_s_search.texts, vector_store_s_search.vectors)\n",
    "\n",
    "print(evaluation_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of 'The fang and key'?\n",
      "Okay, based on the text provided, here's a breakdown of the purpose of the Fang and Key:\n",
      "\n",
      "*   **Ceremonial Significance:** Primarily, the Fang and Key is a ceremonial item. It signifies the PCs' official status as stewards of Willowshore within the jorogumo court of Shenmen. It's a badge of office, indicating they are now part of the town's government and considered rightful rulers.\n",
      "*   **Access to Heh Shan-Bao's Domain:** The most important function is that it unlocks access to Heh Shan-Bao's domain. Specifically, it allows the PCs to enter the restored governor's manor and use the mindscape door within to travel to the corrupted Willowshore mindscape.\n",
      "*   **Unlocking Secrets:** Within the restored manor, the Fang and Key acts as a metaphorical key. It helps the PCs discover secrets and, critically, realize that the Willowshore mindscape still exists, albeit in a corrupted state. It's implied that having the key aids in understanding and interacting with the supernatural elements of the manor.\n",
      "\n",
      "\n",
      "I'm giving this answer a score of 0.8.\n",
      "\n",
      "Strengths:\n",
      "* Correctly identifies the ceremonial significance of the Fang and Key.\n",
      "* Correctly identifies that it unlocks access to Heh Shan-Bao's domain/the restored manor.\n",
      "* Mentions that it helps unlock secrets within the manor.\n",
      "\n",
      "Weaknesses:\n",
      "* Could be more concise.\n",
      "* Doesn't explicitly state that it's required to defeat Heh Shan-Bao\n",
      "* Doesn't state that it is the official mandate from the jorogumo court.\n"
     ]
    }
   ],
   "source": [
    "# Cell for running code to ask questions to the fixed length model. Change # in validation data to number between 0 and 4 for diffrent questions.\n",
    "query = validation_data[4][\"question\"]\n",
    "print(query)\n",
    "\n",
    "response_fixed_length = generate_response(system_prompt, query, vector_store_fixed_length.texts, vector_store_fixed_length.vectors)\n",
    "print(response_fixed_length.text)\n",
    "evaluation_prompt = f\"\"\"Question: {query}\n",
    "Response with fixed-length chunking: {response_fixed_length.text}\n",
    "Ideal answer: {validation_data[0][\"ideal_answer\"]}\"\"\"\n",
    "\n",
    "evaluation_response = generate_response(evaluation_system_prompt, evaluation_prompt, vector_store_fixed_length.texts, vector_store_fixed_length.vectors)\n",
    "\n",
    "print(evaluation_response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
