{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "833d36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fitz\n",
    "import time\n",
    "import polars as pl\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d003fb",
   "metadata": {},
   "source": [
    "### Pre-processing and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "375e1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers_and_footers(\n",
    "    pdf_path, header_height_pt=70, footer_height_pt=70\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, excluding the header and footer areas.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        header_height_pt (int, optional): Height of the header area in points.\n",
    "            Defaults to 50 (a reasonable starting value).\n",
    "        footer_height_pt (int, optional): Height of the footer area in points.\n",
    "            Defaults to 50 (a reasonable starting value).\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text without the header and footer.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        page_rect = page.rect  # Get the page rectangle\n",
    "        page_height = page_rect.height\n",
    "        # Define the clipping rectangle, excluding header and footer\n",
    "        clip_rect = fitz.Rect(\n",
    "            page_rect.x0,\n",
    "            header_height_pt,\n",
    "            page_rect.x1,\n",
    "            page_height - footer_height_pt,\n",
    "        )\n",
    "        text += page.get_text(clip=clip_rect) + \"\\n\"  # Add newline between pages\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "pdf_file_1 = \"test.pdf\"  # Replace with your PDF file\n",
    "pdf_file_2 = \"test_2.pdf\"\n",
    "text_1 = remove_headers_and_footers(pdf_file_1)\n",
    "text_2 = remove_headers_and_footers(pdf_file_2)\n",
    "extracted_text = text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c3e7bbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(extracted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "94061b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into sentences (basic split)\n",
    "sentences = extracted_text.split(\". \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb1970",
   "metadata": {},
   "source": [
    "### Create semantic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f6d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text, model=\"text-embedding-004\", delay = 0.6):\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using OpenAI.\n",
    "\n",
    "    Args:\n",
    "    text (str): Input text.\n",
    "    model (str): Embedding model name.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    time.sleep(delay)\n",
    "    response = client.models.embed_content(\n",
    "              model=model, \n",
    "              contents=text)\n",
    "    \n",
    "    return np.array(response.embeddings[0].values)\n",
    "\n",
    "test = test_get_embedding(\"Describe love\")\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 145 sentence embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each sentence\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} sentence embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "35d7790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compute similarity between consecutive sentences\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9bde0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    Computes chunking breakpoints based on similarity drops.\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): List of similarity scores between sentences.\n",
    "    method (str): 'percentile', 'standard_deviation', or 'interquartile'.\n",
    "    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation').\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices where chunk splits should occur.\n",
    "    \"\"\"\n",
    "    # Determine the threshold value based on the selected method\n",
    "    if method == \"percentile\":\n",
    "        # Calculate the Xth percentile of the similarity scores\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        # Calculate the mean and standard deviation of the similarity scores\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        # Set the threshold value to mean minus X standard deviations\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        # Calculate the first and third quartiles (Q1 and Q3)\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        # Set the threshold value using the IQR rule for outliers\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        # Raise an error if an invalid method is provided\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "\n",
    "    # Identify indices where similarity drops below the threshold value\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "# Compute breakpoints using the percentile method with a threshold of 90\n",
    "breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d0be77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks: 130\n",
      "\n",
      "First text chunk:\n",
      "28623831\n",
      "4649393\n",
      "The Summer \n",
      "That Never \n",
      "Was\n",
      "Campaign \n",
      "Overview\n",
      "Chapter 1:  \n",
      "To Light the \n",
      "Night\n",
      "Chapter 2:  \n",
      "Reclaiming \n",
      "Willowshore\n",
      "Chapter 3:  \n",
      "The \n",
      "Willowshore \n",
      "Curse\n",
      "Chapter 4:  \n",
      "The Wall of \n",
      "Ghosts\n",
      "Willowshore\n",
      "Adventure  \n",
      "Toolbox\n",
      "less important than defeating the creatures that now \n",
      "control downtown Willowshore.\n",
      "Ugly Cute: Granny Hu did hear from some survivors \n",
      "that the spider statue has gone missing, but she finds \n",
      "the idea of the stone statue “waking up on \n",
      "its own” to be ridiculous superstition.\n"
     ]
    }
   ],
   "source": [
    "def split_into_chunks(sentences, breakpoints):\n",
    "    \"\"\"\n",
    "    Splits sentences into semantic chunks.\n",
    "\n",
    "    Args:\n",
    "    sentences (List[str]): List of sentences.\n",
    "    breakpoints (List[int]): Indices where chunking should occur.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    start = 0  # Initialize the start index\n",
    "\n",
    "    # Iterate through each breakpoint to create chunks\n",
    "    for bp in breakpoints:\n",
    "        # Append the chunk of sentences from start to the current breakpoint\n",
    "        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n",
    "        start = bp + 1  # Update the start index to the next sentence after the breakpoint\n",
    "\n",
    "    # Append the remaining sentences as the last chunk\n",
    "    chunks.append(\". \".join(sentences[start:]))\n",
    "    return chunks  # Return the list of chunks\n",
    "\n",
    "# Create chunks using the split_into_chunks function\n",
    "text_chunks = split_into_chunks(sentences, breakpoints)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Number of semantic chunks: {len(text_chunks)}\")\n",
    "\n",
    "# Print the first chunk to verify the result\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    Creates embeddings for each text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    # Generate embeddings for each text chunk using the get_embedding function\n",
    "    return [get_embedding(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Create chunk embeddings using the create_embeddings function\n",
    "chunk_embeddings = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb45e2",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3cd17e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    #tagit bort np.array framför embedding i append\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def semantic_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\"text\": self.texts[idx],\n",
    "                            \"metadata\": self.metadata[idx],\n",
    "                            \"similarity\": score\n",
    "                            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, filename=\"embeddings.parquet\"):\n",
    "        df = pl.DataFrame(\n",
    "            dict(\n",
    "                vectors=self.vectors,\n",
    "                texts=self.texts,\n",
    "                metadata=self.metadata))\n",
    "        df.write_parquet(filename)\n",
    "\n",
    "    def load(self, file):\n",
    "        df = pl.read_parquet(file, columns=[\"vectors\", \"texts\", \"metadata\"])\n",
    "        self.vectors = df[\"vectors\"].to_list()\n",
    "        self.texts = df[\"texts\"].to_list()\n",
    "        self.metadata = df[\"metadata\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fa117199",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "for sentence in sentences:\n",
    "    vector_store.add_item(text=sentence, embedding=chunk_embeddings, metadata=[\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4ca0fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 768)\n"
     ]
    }
   ],
   "source": [
    "print((vector_store.vectors[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "eb1c1164",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot parse numpy data type dtype('O') into Polars data type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[258]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[255]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mVectorStore.save\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename=\u001b[33m\"\u001b[39m\u001b[33membeddings.parquet\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     df.write_parquet(filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\karlt\\Documents\\ec_utbildning\\DeepLearning\\.venv\\Lib\\site-packages\\polars\\dataframe\\frame.py:365\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, schema, schema_overrides, strict, orient, infer_schema_length, nan_to_null)\u001b[39m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m._df = dict_to_pydf(\n\u001b[32m    361\u001b[39m         {}, schema=schema, schema_overrides=schema_overrides\n\u001b[32m    362\u001b[39m     )\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28mself\u001b[39m._df = \u001b[43mdict_to_pydf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, Sequence)):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m._df = sequence_to_pydf(\n\u001b[32m    375\u001b[39m         data,\n\u001b[32m    376\u001b[39m         schema=schema,\n\u001b[32m   (...)\u001b[39m\u001b[32m    381\u001b[39m         nan_to_null=nan_to_null,\n\u001b[32m    382\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\karlt\\Documents\\ec_utbildning\\DeepLearning\\.venv\\Lib\\site-packages\\polars\\_utils\\construction\\dataframe.py:159\u001b[39m, in \u001b[36mdict_to_pydf\u001b[39m\u001b[34m(data, schema, schema_overrides, strict, nan_to_null, allow_multithreaded)\u001b[39m\n\u001b[32m    146\u001b[39m     data_series = [\n\u001b[32m    147\u001b[39m         pl.Series(\n\u001b[32m    148\u001b[39m             name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m column_names\n\u001b[32m    155\u001b[39m     ]\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m     data_series = [\n\u001b[32m    158\u001b[39m         s._s\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_expand_dict_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.values()\n\u001b[32m    165\u001b[39m     ]\n\u001b[32m    167\u001b[39m data_series = _handle_columns_arg(data_series, columns=column_names, from_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m pydf = PyDataFrame(data_series)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\karlt\\Documents\\ec_utbildning\\DeepLearning\\.venv\\Lib\\site-packages\\polars\\_utils\\construction\\dataframe.py:388\u001b[39m, in \u001b[36m_expand_dict_values\u001b[39m\u001b[34m(data, schema_overrides, strict, order, nan_to_null)\u001b[39m\n\u001b[32m    385\u001b[39m     updated_data[name] = s\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arrlen(val) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m _is_generator(val):\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     updated_data[name] = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(  \u001b[38;5;66;03m# type: ignore[redundant-expr]\u001b[39;00m\n\u001b[32m    396\u001b[39m     val, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, date, datetime, time, timedelta)\n\u001b[32m    397\u001b[39m ):\n\u001b[32m    398\u001b[39m     updated_data[name] = F.repeat(\n\u001b[32m    399\u001b[39m         val, array_len, dtype=dtype, eager=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    400\u001b[39m     ).alias(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\karlt\\Documents\\ec_utbildning\\DeepLearning\\.venv\\Lib\\site-packages\\polars\\series\\series.py:293\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, name, values, dtype, strict, nan_to_null)\u001b[39m\n\u001b[32m    290\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, Sequence):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28mself\u001b[39m._s = \u001b[43msequence_to_pyseries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28mself\u001b[39m._s = sequence_to_pyseries(name, [], dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\karlt\\Documents\\ec_utbildning\\DeepLearning\\.venv\\Lib\\site-packages\\polars\\_utils\\construction\\series.py:293\u001b[39m, in \u001b[36msequence_to_pyseries\u001b[39m\u001b[34m(name, values, dtype, strict, nan_to_null)\u001b[39m\n\u001b[32m    289\u001b[39m srs = PySeries.new_from_any_values(name, values, strict)\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _check_for_numpy(python_dtype, check_type=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    291\u001b[39m     np.bool_(\u001b[38;5;28;01mTrue\u001b[39;00m), np.generic\n\u001b[32m    292\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     dtype = \u001b[43mnumpy_char_code_to_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpython_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m srs.cast(dtype, strict=strict, wrap_numerical=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\karlt\\Documents\\ec_utbildning\\DeepLearning\\.venv\\Lib\\site-packages\\polars\\datatypes\\convert.py:334\u001b[39m, in \u001b[36mnumpy_char_code_to_dtype\u001b[39m\u001b[34m(dtype_char)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    333\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcannot parse numpy data type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m into Polars data type\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot parse numpy data type dtype('O') into Polars data type"
     ]
    }
   ],
   "source": [
    "vector_store.save(\"embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2042f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_2 = VectorStore()\n",
    "vector_store_2.load(\"embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc956e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91b57311",
   "metadata": {},
   "source": [
    "### New semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae726427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_2(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d411c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2e887f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(query):\n",
    "    context = \"\\n\".join(semantic_search(query, text_chunks, chunk_embeddings))\n",
    "    user_prompt = f\"The question is {query}. This is the context: {context}.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0ace21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        config=types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt),\n",
    "        contents=generate_user_prompt(user_message)\n",
    "        )\n",
    "    return response\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a376556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Granny Hu's objectives are:\n",
      "\n",
      "1.  To ensure that Willowshore still has a physician when everything is over.\n",
      "2.  To retake the barracks downtown for its armory.\n",
      "3.  To find her missing grandchildren.\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(system_prompt, \"What is granny Hu's objective?\").text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
